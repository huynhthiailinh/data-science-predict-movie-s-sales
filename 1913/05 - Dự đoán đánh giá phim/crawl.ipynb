{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mục đích mà bài toán hướng tới và phương hướng giải quyết\n",
    "---\n",
    "- Bài toán: Dự đoán số điểm đánh giá một bộ phim trên thang điểm 0 tới 10 dựa trên IMDB_Rating.\n",
    "- Mục tiêu:\n",
    "    - Bài toán chung đặt ra bao gồm:\n",
    "        - Crawl dữ liệu các phim và trích xuất các dữ liệu cần thiết để có thể dự đoán bộ phim có điểm số bao nhiêu.\n",
    "        - Minh họa dữ liệu để xem các đặt trưng của dữ liệu đó thế nào để phục vụ cho công việc tiếp theo.\n",
    "        - Phân tích các đặt trưng dựa EDA và thực hiện các Feature Engineering với các hành động như bổ sung đặc trưng bị trống, biến đổi (Label hóa) dữ liệu về dạng số để thực hiện bài toán dự đoán.\n",
    "        - Dự đoán điểm bộ phim đó như thế nào dựa trên các đặc trưng mà bộ phim có."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1. Crawl data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trang web mà chúng ta sẽ sử dụng để crawl dữ liệu: https://reelgood.com/movies <br /> <br />\n",
    "<img style=\"display: block; width: 80%; object-fit: cover; margin: auto\" src=\"https://res.cloudinary.com/toanil315/image/upload/v1656231528/Screenshot_2712_pbnhjv.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đầu tiên chúng ta sẽ tiến hành lấy link của các bộ phim. Chúng ta sẽ tiến hành lấy dữ liệu phim ở trang detail của chúng để đảm bảo có đủ thông tin nhất có thể. Chúng ta vẫn có thể lấy dữ liệu ở trang list nhưng sẽ thiếu các thông tin (description, director, genre...) <br /> <br />\n",
    "<img style=\"display: block; width: 80%; object-fit: cover; margin: auto\" src=\"https://res.cloudinary.com/toanil315/image/upload/v1656231615/Screenshot_2713_t0khcw.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#link of films in reelgood.com\n",
    "for i in range(0, 5500, 50):\n",
    "    URL= \"https://reelgood.com/movies?offset=\" + str(i)\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "    f = open(\"movies.txt\", \"a\")\n",
    "    for link in soup.select('[itemprop=itemListElement] [itemprop=url]'):\n",
    "        data = link.get('content')\n",
    "        f.write(data)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiến hành tạo ra một object mẫu của film"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filmObj = {\n",
    "      \"Title\": None,\n",
    "      \"IMDB_Rating\": None,\n",
    "      \"Reelgood_Rating\": None,\n",
    "      \"Genres\": [],\n",
    "      \"Year\": None,\n",
    "      \"Tags\": [],\n",
    "      \"Country\": None,\n",
    "      \"Source\": None,\n",
    "      \"Rated\": None,\n",
    "      \"Duration\": None,\n",
    "      \"Description\": None,\n",
    "      \"Director\": None,\n",
    "      \"Employees\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiến hành lấy dữ liệu bằng cách request tới link của các bộ phim trong tập chúng ta vừa lấy được lúc nãy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('movies.txt') as f:\n",
    "    links = [line.rstrip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_film = []\n",
    "for i in range(0, len(links)):\n",
    "  link = links[i]\n",
    "  film = copy.deepcopy(filmObj)\n",
    "\n",
    "  try:\n",
    "    with requests.get(link) as page:\n",
    "      soup = BeautifulSoup(page.content, 'html.parser')\n",
    "      film['Title'] = soup.find(class_=\"css-of585f e14injhv7\")\n",
    "      if film['Title'] != None:\n",
    "        film['Title'] = film['Title'].get_text()\n",
    "\n",
    "      div_info = soup.find(class_=\"css-1su90nd ey4ir3j0\")\n",
    "\n",
    "      # GET SCORE\n",
    "      film['IMDB_Rating'] = div_info.find(class_=\"css-xmin1q ey4ir3j3\").get_text()\n",
    "      film['Reelgood_Rating'] = div_info.find(class_=\"css-xmin1q ey4ir3j8\")\n",
    "      if film['Reelgood_Rating'] != None:\n",
    "        film['Reelgood_Rating'] = film['Reelgood_Rating'].get_text()\n",
    "\n",
    "      # GET INFO BY TAG A\n",
    "      list_info = div_info.find_all(class_=\"css-10wrqt0\")\n",
    "      for info in list_info:\n",
    "        type_info = info.get(\"href\")\n",
    "        if('/genre' in type_info):\n",
    "          film['Genres'].append(info.get_text())\n",
    "        elif '/year' in type_info:\n",
    "          film['Year'] = info.get_text()\n",
    "        elif '/list' in type_info: \n",
    "          film['Tags'].append(info.get_text())\n",
    "        elif '/origin' in type_info:\n",
    "          film['Country'] = info.get_text()\n",
    "        elif '/source' in type_info:\n",
    "          film['Source'] = info.get_text()\n",
    "\n",
    "      # GET INFO BY TAG META\n",
    "      film['Rated'] = div_info.find('meta', itemprop='contentRating')\n",
    "      if film['Rated'] != None:\n",
    "        film['Rated'] = film['Rated'].get(\"content\", None)\n",
    "      film['Duration'] = div_info.find('meta', itemprop='duration')\n",
    "      if film['Duration'] != None: \n",
    "        film['Duration'] = film['Duration'].get(\"content\", None)\n",
    "\n",
    "      # GET DESCRIPTION\n",
    "      film['Description'] = soup.find('p', itemprop=\"description\").get_text()\n",
    "\n",
    "      # GET CAST & CREW\n",
    "      list_employee = soup.find_all('h3', itemprop=\"name\", class_=\"css-11yr18h egg5eqo2\")\n",
    "      # GET DIRECTOR\n",
    "      if len(list_employee) > 0:\n",
    "        film['Director'] = list_employee[0].get_text()\n",
    "        #GET EMPLOYEE\n",
    "        for i in range(1, len(list_employee)):\n",
    "          employee = list_employee[i]\n",
    "          film['Employees'].append(employee.get_text())\n",
    "\n",
    "      print(link, \" done\")\n",
    "      arr_film.append(film)\n",
    "\n",
    "  except (NameError, AttributeError) as e:\n",
    "    print(e)\n",
    "    continue\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tiến hành convert thành dataframe và lưu dưới dạng file csv\n",
    "<p >\n",
    "    <span style=\"text-decoration: underline\">Lưu ý</span>: <br />\n",
    "    Khi request thì có một số link bị hỏng hoặc tiêu đề của film được đánh class sai, do đó sẽ xuất hiện tình trạng không thể lấy dữ liệu và request timeout gây ra vấn đề bị ngắt khi đang lấy. Chúng ta có thể xử lý bằng cách crawl thành từng file nhỏ và ghép chúng lại thành 1 dataframe tổng sau khi đã chạy hết links đã lấy\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(arr_film)\n",
    "df.to_csv('film_reelgood.csv',index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta có thể để ý rằng trong đặc trưng Description, thường ở phần cuối nó sẽ có số luợng người đánh giá bộ phim, có thể đặc trưng này sẽ quan trọng nên chúng ta cần tiến hành trích xuất nó ra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_film = pd.read_csv(\"film_reelgood.csv\", encoding=\"utf-8\")\n",
    "df_film.loc[0]['Description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_votes = []\n",
    "\n",
    "# extract user votes\n",
    "for i in df_film['Description']:\n",
    "    sub_str = i[-50:]\n",
    "    startIndex = sub_str.find(\"(\")\n",
    "    endIndex = sub_str.find(\")\")\n",
    "    if('votes' in sub_str[startIndex: endIndex]):\n",
    "        arr_votes.append(sub_str[startIndex + 1: endIndex])\n",
    "    else: arr_votes.append(np.NaN)\n",
    "\n",
    "# concat into df and save as file csv\n",
    "arr_votes = np.array(arr_votes)\n",
    "df_votes = pd.DataFrame(arr_votes, columns = ['User_Votes'])\n",
    "result = pd.concat([df_film, df_votes], axis=1)\n",
    "result.to_csv(\"raw_data.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kết quả cuối cùng:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_film = pd.read_csv(\"raw_data.csv\", encoding=\"utf-8\")\n",
    "df_film.info()\n",
    "df_film.head(2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
