{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhOPnHZO2kEW"
      },
      "source": [
        "# CHƯƠNG TRÌNH CRAWL DỮ LIỆU DOANH THU PHIM\n",
        "\n",
        "[<img src=\"https://cdn.searchenginejournal.com/wp-content/uploads/2020/12/59c68dd6-448c-449d-ac0a-db5c83c38526-5fc862dca57c4-1520x800.jpeg\">](Thumbnail)\n",
        "\n",
        "### Mô tả chương trình\n",
        "\n",
        "<details>\n",
        "<summary><b>Mục đích crawling dữ liệu</b></summary>\n",
        "<ul>\n",
        "  <li>\n",
        "    Thu thập dữ liệu là quá trình tự động trích xuất các thông tin từ các trang web và lưu trữ nó dưới một định dạng phù hợp.\n",
        "  </li>\n",
        "  <li>\n",
        "    Thông thường, khi muốn lấy một số thông tin từ các trang web, chúng ta sẽ dùng các API mà các trang đó cung cấp.\n",
        "  </li>\n",
        "  <li>\n",
        "    Đây là cách đơn giản, tuy nhiên không phải trang web nào cũng cung cấp sẵn API cho chúng ta sử dụng. Do đó chúng ta cần một kỹ thuật để lấy các thông tin từ các trang web đó mà không thông qua API.\n",
        "  </li>\n",
        "</ul>\n",
        "</details>\n",
        "<details>\n",
        "<summary><b>Lưy ý</b></summary>\n",
        "<ul>\n",
        "  <li>\n",
        "    Ngân sách của một bộ phim rất khó tìm và nếu có cũng không đáng tin, vì các hãng làm phim thường giữ bí mật thông tin. Các nhà sản xuất phim thường công bố ngân sách sau khi đã tăng hoặc giảm so với thực tế.\n",
        "  </li>\n",
        "  <li>\n",
        "    Tuy vậy, ở phần Movie Budgets trên trang web The Numbers [1] thống kê một bảng ngân sách chính xác của các bộ phim mà họ thu thập được, tuy nhiên cũng có những chỗ trống và những chỗ số liệu còn đang tranh cãi.\n",
        "  </li>\n",
        "</ul>\n",
        "</details>\n",
        "\n",
        "#### Đường dẫn đến nguồn dữ liệu: *https://www.the-numbers.com/movie/budgets/all*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_tekwwk4XXZ"
      },
      "source": [
        "## Bước 1: Cấu hình môi trường và import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "TUuyNETWdFkJ"
      },
      "outputs": [],
      "source": [
        "from urllib.request import Request, urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import io"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmcRH5474ktV"
      },
      "source": [
        "## Bước 2: Khởi tạo file csv tiền lưu trữ dữ liệu được crawl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "4MYkFMcWz2_O"
      },
      "outputs": [],
      "source": [
        "file_name = 'raw-data.csv'\n",
        "writer = csv.writer(io.open(file_name, 'w', encoding='utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3va5pQ1M_V9v"
      },
      "source": [
        "Khởi tạo mảng các nhóm dữ liệu trên 1 trang ( 100 mẫu mỗi trang cần crawl )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yqtpblip0VEh",
        "outputId": "d3b1e2f2-1e04-44db-aff4-eb7c4c552af1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "171"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pages = ['', '/101', '/201', '/301', '/401', '/501', '/601', '/701', '/801', '/901',\n",
        "        '/1001', '/1101', '/1201', '/1301', '/1401', '/1501', '/1601', '/1701', '/1801', '/1901',\n",
        "        '/2001', '/2101', '/2201', '/2301', '/2401', '/2501', '/2601', '/2701', '/2801', '/2901',\n",
        "        '/3001', '/3101', '/3201', '/3301', '/3401', '/3501', '/3601', '/3701', '/3801', '/3901',\n",
        "        '/4001', '/4101', '/4201', '/4301', '/4401', '/4501', '/4601', '/4701', '/4801', '/4901',\n",
        "        '/5001', '/5101', '/5201', '/5301', '/5401', '/5501', '/5601', '/5701', '/5801', '/5901',\n",
        "        '/6001', '/6101', '/6201']\n",
        "\n",
        "# Các đặc trưng\n",
        "writer.writerow(['', 'ReleaseDate', 'Movie', 'RunningTime', 'Source', 'Genre', 'ProductionMethod',\n",
        "                'CreativeType', 'ProductionCompanies', 'ProductionCountries', 'Languages',\n",
        "                'ProductionBudget', 'DomesticGross', 'WorldwideGross'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TybZlewj41Q0"
      },
      "source": [
        "## Bước 3: Thiết kế hàm xử lý dữ liệu trong các trang thu được\n",
        "\n",
        "<details>\n",
        "<summary>\n",
        "  <b>Phương thức crawl dữ liệu</b>\n",
        "</summary>\n",
        "<ul>\n",
        "  <li>\n",
        "    Mục tiêu là từ trang Movie Budgets, lấy tất cả các thông tin ở bảng của từng trang. Với mỗi dòng dữ liệu, ta sẽ truy cập vào trang chi tiết để lấy thông tin bảng Moive Details của từng bộ phim.\n",
        "  </li>\n",
        "  <li>\n",
        "    Xem page source của trang web, phân tích được dữ liệu cần lấy nằm trong bảng. Mỗi trang chỉ có một bảng, nên chỉ cần duyệt qua từng trang, mỗi trang ta duyệt qua từng hàng bằng câu lệnh <code>for tr in soup.find_all('tr')</code> và duyệt qua từng ô trong hàng bằng câu lệnh <code>for td in tr.find_all('td')</code>.\n",
        "  </li>\n",
        "  <li>\n",
        "    Riêng trong ô ở cột “Movie” là thẻ a chứa url dẫn đến trang chi tiết của từng bộ phim, gọi hàm <code>a = td.select_one('b > a')</code> để lấy được đường dẫn trong thuộc tính <href>. Vì trang chi tiết chứa nhiều bảng, ta sẽ tìm bảng bằng cách tìm kiếm phần tử thẻ \"h2\" có text là “Movie Details” bằng câu lệnh <code>h2 = detail_soup.find('h2', text='Movie Details')</code>, sau đó tìm bảng nằm ngay sau thẻ “h2” đó bằng câu lệnh <code>table = h2.find_next_sibling()</code> , tương tự lấy những cột dữ liệu ta cần.\n",
        "  </li>\n",
        "  <li>\n",
        "    Ghi dữ liệu vào tệp csv sử dụng module <code>csv</code> [3]. Đầu tiên, mở tệp csv để ghi (chế độ w) sử dụng hàm <code>open()</code>. Tiếp theo, tạo một CSV writer bằng cách gọi hàm <code>writer()</code> của module csv. Ghi dữ liệu vào tệp csv dùng hàm <code>writerow()</code> hoặc <code>writerows()</code> của đối tượng CSV writer. Cuối cùng, đóng tệp khi hoàn thành việc ghi dữ liệu.\n",
        "  </li>\n",
        "</ul>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "uzOBt1DJ0thI"
      },
      "outputs": [],
      "source": [
        "def insert_data(page):\n",
        "    url = 'https://www.the-numbers.com/movie/budgets/all' + page\n",
        "\n",
        "    req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "    webpage = urlopen(req).read()\n",
        "    soup = BeautifulSoup(webpage, 'html.parser')\n",
        "\n",
        "    for tr in soup.find_all('tr'):\n",
        "        data = []\n",
        "        for td in tr.find_all('td'):\n",
        "            data.append(td.text.strip().replace('$','').replace(',',''))\n",
        "            a = td.select_one('b > a')\n",
        "            if a:\n",
        "                href = a['href']\n",
        "                detail_url = 'https://www.the-numbers.com' + href\n",
        "\n",
        "                detail_req = Request(detail_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "                detail_webpage = urlopen(detail_req).read()\n",
        "                detail_soup = BeautifulSoup(detail_webpage, 'html.parser')\n",
        "\n",
        "                h2 = detail_soup.body.find('h2', text='Movie Details')\n",
        "                if h2:\n",
        "                    table = h2.find_next_sibling()\n",
        "\n",
        "                    running_time = table.find('td', text='Running Time:')\n",
        "                    if running_time:\n",
        "                        running_time = running_time.find_next_sibling().text\n",
        "                        data.append(running_time.strip())\n",
        "                    else:\n",
        "                        data.append('')\n",
        "\n",
        "                    source = table.find('td', text='Source:')\n",
        "                    if source:\n",
        "                        source = source.find_next_sibling().text\n",
        "                        data.append(source.strip())\n",
        "                    else:\n",
        "                        data.append('')\n",
        "\n",
        "                    genre = table.find('td', text='Genre:')\n",
        "                    if genre:\n",
        "                        genre = genre.find_next_sibling().text\n",
        "                        data.append(genre.strip())\n",
        "                    else:\n",
        "                        data.append('')\n",
        "\n",
        "                    production_method = table.find('td', text='Production Method:')\n",
        "                    if production_method:\n",
        "                        production_method = production_method.find_next_sibling().text\n",
        "                        data.append(production_method.strip())\n",
        "                    else:\n",
        "                        data.append('')\n",
        "\n",
        "                    creative_type = table.find('td', text='Creative Type:')\n",
        "                    if creative_type:\n",
        "                        creative_type = creative_type.find_next_sibling().text\n",
        "                        data.append(creative_type.strip())\n",
        "                    else:\n",
        "                        data.append('')\n",
        "\n",
        "                    production_companies = table.find('td', text='Production/Financing Companies:')\n",
        "                    if production_companies:\n",
        "                        production_companies = production_companies.find_next_sibling().text\n",
        "                        data.append(production_companies.strip())\n",
        "                    else:\n",
        "                        data.append('')\n",
        "\n",
        "                    production_countries = table.find('td', text='Production Countries:')\n",
        "                    if production_countries:\n",
        "                        production_countries = production_countries.find_next_sibling().text\n",
        "                        data.append(production_countries.strip())\n",
        "                    else:\n",
        "                        data.append('')\n",
        "\n",
        "                    languages = table.find('td', text='Languages:')\n",
        "                    if languages:\n",
        "                        languages = languages.find_next_sibling().text\n",
        "                        data.append(languages.strip())\n",
        "                    else:\n",
        "                        data.append('')\n",
        "\n",
        "        if(data):\n",
        "            print(\"Inserting row: {}\".format(','.join(data)))\n",
        "            writer.writerow(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qPp6Fxy5vH8"
      },
      "source": [
        "## Bước 4: Tiến hành crawl dữ liệu của từng trang\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nOy4HQMB1YiH",
        "outputId": "427bb908-f7fe-41a0-a845-f960db11ece1"
      },
      "outputs": [],
      "source": [
        "for page in pages:\n",
        "    insert_data(page)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "crawler.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
